Project: HR1 Semantic Q&A Assistant Goal
Build a minimal viable product (MVP) for a web-based chat interface that allows natural language querying of the HR1 legislative bill. The system will use semantic search (via vector embeddings) and LLM-generated answers based only on retrieved HR1 content.

Project Structure
‚Ä¢ Frontend: Next.js (deployed via Vercel)
‚Ä¢ Backend: Next.js API routes (Node.js / TypeScript)
‚Ä¢ Vector Store: Pinecone (preferred), with optional fallback to Supabase pgvector
‚Ä¢ Embedding Model: OpenAI text-embedding-ada-002
‚Ä¢ LLM for Q&A: OpenAI o3 (latest reasoning model)
‚Ä¢ Data Format: Plain text chunks of HR1 with metadata

Tasks

üóÇ Project Setup
‚Ä¢ [ ] Create a new Next.js 13+ project with TypeScript
‚Ä¢ [ ] Initialize git repository and create .gitignore
‚Ä¢ [ ] Configure .env.local with API keys for OpenAI and Pinecone
‚Ä¢ [ ] Create .env.example with placeholder values for documentation
‚Ä¢ [ ] Install required packages: openai, @pinecone-database/pinecone, @vercel/ai, react-markdown, dotenv
‚Ä¢ [ ] Install dev dependencies: @types/node, eslint, prettier
‚Ä¢ [ ] Set up Tailwind CSS for styling (optional but recommended)
‚Ä¢ [ ] Configure TypeScript strict mode and ESLint rules
‚Ä¢ [ ] Create /data/hr1.txt with plain text version of the bill
‚Ä¢ [ ] Set up basic error boundary component for React

üìä HR1 Document Ingestion
‚Ä¢ [ ] Create scripts/chunk_and_embed.ts
‚Ä¢ [ ] Implement text preprocessing (remove extra whitespace, normalize formatting)
‚Ä¢ [ ] Load HR1 text from /data/hr1.txt
‚Ä¢ [ ] Split into ~500 token chunks with ~50 token overlap
‚Ä¢ [ ] Implement smart chunking that respects sentence boundaries
‚Ä¢ [ ] Use OpenAI embeddings to vectorize each chunk
‚Ä¢ [ ] Add retry logic for API calls with exponential backoff
‚Ä¢ [ ] Upsert chunks to Pinecone with metadata (chunk ID, text, section heading, page number, chunk position)
‚Ä¢ [ ] Implement progress bar for chunking process
‚Ä¢ [ ] Log progress and errors for verification
‚Ä¢ [ ] Create validation script to verify all chunks were properly embedded
‚Ä¢ [ ] Add dry-run mode for testing without API calls

üîç Semantic Search + RAG API
‚Ä¢ [ ] Implement /app/api/chat/route.ts as POST endpoint
‚Ä¢ [ ] Add input validation and sanitization
‚Ä¢ [ ] Implement rate limiting middleware (e.g., using upstash/ratelimit)
‚Ä¢ [ ] Accept user query as input
‚Ä¢ [ ] Add query length limits and validation
‚Ä¢ [ ] Embed user query with OpenAI
‚Ä¢ [ ] Implement caching for repeated queries
‚Ä¢ [ ] Perform similarity search in Pinecone
‚Ä¢ [ ] Add fallback to Supabase pgvector if Pinecone fails
‚Ä¢ [ ] Retrieve top 3‚Äì5 relevant chunks
‚Ä¢ [ ] Implement relevance threshold to filter low-quality matches
‚Ä¢ [ ] Construct a prompt with retrieved context and question
‚Ä¢ [ ] Add prompt engineering for better citation formatting
‚Ä¢ [ ] Call OpenAI GPT for streaming completion
‚Ä¢ [ ] Implement timeout handling for long-running requests
‚Ä¢ [ ] Return assistant response along with list of cited HR1 chunks
‚Ä¢ [ ] Add response time logging and monitoring

üí¨ Frontend Chat UI
‚Ä¢ [ ] Create ChatPage in /app/page.tsx
‚Ä¢ [ ] Implement responsive design for mobile/tablet/desktop
‚Ä¢ [ ] Display user/assistant messages in chat format
‚Ä¢ [ ] Add loading states with skeleton UI
‚Ä¢ [ ] Add error states with user-friendly messages
‚Ä¢ [ ] Input box for entering questions
‚Ä¢ [ ] Add character limit indicator
‚Ä¢ [ ] Implement "Enter to send, Shift+Enter for new line"
‚Ä¢ [ ] Use @vercel/ai useChat for request/response streaming
‚Ä¢ [ ] Add abort functionality for in-progress requests
‚Ä¢ [ ] Render assistant responses using react-markdown
‚Ä¢ [ ] Add syntax highlighting for any code blocks
‚Ä¢ [ ] Display citations below assistant messages with expandable section text
‚Ä¢ [ ] Add "Copy to clipboard" for responses
‚Ä¢ [ ] Implement chat history with localStorage
‚Ä¢ [ ] Add "Clear chat" functionality
‚Ä¢ [ ] Add accessibility features (ARIA labels, keyboard navigation)
‚Ä¢ [ ] Implement auto-scroll to latest message

üß™ Testing & Quality Assurance
‚Ä¢ [ ] Set up Jest and React Testing Library
‚Ä¢ [ ] Write unit tests for chunking algorithm
‚Ä¢ [ ] Write unit tests for API endpoint logic
‚Ä¢ [ ] Add integration tests for chat flow
‚Ä¢ [ ] Test error scenarios (API failures, rate limits)
‚Ä¢ [ ] Add performance benchmarks for search
‚Ä¢ [ ] Create test dataset of common HR1 questions

üîí Security & Performance
‚Ä¢ [ ] Implement CORS configuration
‚Ä¢ [ ] Add security headers (CSP, X-Frame-Options, etc.)
‚Ä¢ [ ] Set up API request logging (without sensitive data)
‚Ä¢ [ ] Implement cost tracking for OpenAI API usage
‚Ä¢ [ ] Add monitoring for API response times
‚Ä¢ [ ] Set up error tracking (e.g., Sentry)
‚Ä¢ [ ] Implement request throttling per user/IP

üöÄ Deployment
‚Ä¢ [ ] Create production environment variables
‚Ä¢ [ ] Deploy site to Vercel
‚Ä¢ [ ] Configure custom domain (if applicable)
‚Ä¢ [ ] Add required secrets (OpenAI, Pinecone keys) to Vercel environment
‚Ä¢ [ ] Set up preview deployments for PRs
‚Ä¢ [ ] Test chat interface end-to-end
‚Ä¢ [ ] Validate citation traceability
‚Ä¢ [ ] Create basic monitoring dashboard
‚Ä¢ [ ] Set up uptime monitoring

üìö Documentation
‚Ä¢ [ ] Create README.md with setup instructions
‚Ä¢ [ ] Document API endpoints and request/response formats
‚Ä¢ [ ] Add inline code comments for complex logic
‚Ä¢ [ ] Create user guide for the chat interface
‚Ä¢ [ ] Document chunking strategy and parameters

Stretch Goals (Optional)
‚Ä¢ [ ] Claude integration toggle
‚Ä¢ [ ] Sidebar or inline full HR1 viewer
‚Ä¢ [ ] Chat UI enhancements (e.g., citation highlighting)
‚Ä¢ [ ] Admin-only ingestion trigger (for future docs)
‚Ä¢ [ ] Export chat history as PDF/Markdown
‚Ä¢ [ ] Add voice input support
‚Ä¢ [ ] Implement semantic caching for similar queries
‚Ä¢ [ ] Add analytics for popular questions
‚Ä¢ [ ] Multi-language support
‚Ä¢ [ ] Dark mode toggle

Phase 2: Senatorial Value Alignment Layer (Optional)
Purpose
Compare the content of HR1 with campaign promises or stated values from each currently sitting Senate member, based on content scraped from their official or campaign websites.

Tasks
‚Ä¢ [ ] Scrape senate.gov member list and individual pages
‚Ä¢ [ ] Extract value/policy statements from .gov or campaign domains
‚Ä¢ [ ] Embed statements using OpenAI embeddings
‚Ä¢ [ ] Store with metadata: { name, party, district, text, url }
‚Ä¢ [ ] Enable semantic comparison between HR1 sections and member values
‚Ä¢ [ ] Add UI to explore alignment/conflict by rep, party, state, or issue

Future Enhancements
‚Ä¢ [ ] Score alignment/conflict as a semantic distance metric
‚Ä¢ [ ] Offer summary per representative's consistency with HR1
‚Ä¢ [ ] Track document freshness or cache updates

Notes
‚Ä¢ Use AWS credits only if Pinecone or embedding workloads exceed free tiers
‚Ä¢ Chunking and ingestion is one-time; no recurring job needed
‚Ä¢ Citations must be shown with each model response for transparency
‚Ä¢ Text chunks should be capped to ~1000 tokens with overlap to maintain coherence
‚Ä¢ Consider implementing a feedback mechanism for answer quality
‚Ä¢ Monitor costs closely, especially during development

Next Steps
‚Ä¢ [ ] Finalize clean HR1 plain text file
‚Ä¢ [ ] Run chunking + embedding script once to populate Pinecone index
‚Ä¢ [ ] Implement API route to power question answering
‚Ä¢ [ ] Build chat UI and test citation formatting
‚Ä¢ [ ] Conduct user testing with sample queries
