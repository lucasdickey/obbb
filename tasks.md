Project: HR1 Semantic Q&A Assistant Goal
Build a minimal viable product (MVP) for a web-based chat interface that allows natural language querying of the HR1 legislative bill. The system will use semantic search (via vector embeddings) and LLM-generated answers based only on retrieved HR1 content.

Project Structure
‚Ä¢ Frontend: Next.js (deployed via Vercel)
‚Ä¢ Backend: Next.js API routes (Node.js / TypeScript)
‚Ä¢ Vector Store: Pinecone (preferred), with optional fallback to Supabase pgvector
‚Ä¢ Embedding Model: OpenAI text-embedding-ada-002
‚Ä¢ LLM for Q&A: OpenAI o3 (latest reasoning model)
‚Ä¢ Data Format: Plain text chunks of HR1 with metadata

Tasks

üóÇ Project Setup
‚Ä¢ [ ] Create a new Next.js 13+ project with TypeScript
‚Ä¢ [ ] Initialize git repository and create .gitignore
‚Ä¢ [ ] Configure .env.local with API keys for OpenAI and Pinecone
‚Ä¢ [ ] Create .env.example with placeholder values for documentation
‚Ä¢ [ ] Install required packages: openai, @pinecone-database/pinecone, @vercel/ai, react-markdown, dotenv
‚Ä¢ [ ] Install dev dependencies: @types/node, eslint, prettier
‚Ä¢ [ ] Set up Tailwind CSS for styling (optional but recommended)
‚Ä¢ [ ] Configure TypeScript strict mode and ESLint rules
‚Ä¢ [ ] Create /data/hr1.txt with plain text version of the bill
‚Ä¢ [ ] Set up basic error boundary component for React

üìä HR1 Document Ingestion
‚Ä¢ [ ] Create scripts/chunk_and_embed.ts
‚Ä¢ [ ] Implement text preprocessing (remove extra whitespace, normalize formatting)
‚Ä¢ [ ] Load HR1 text from /data/hr1.txt
‚Ä¢ [ ] Split into ~500 token chunks with ~50 token overlap
‚Ä¢ [ ] Implement smart chunking that respects sentence boundaries
‚Ä¢ [ ] Use OpenAI embeddings to vectorize each chunk
‚Ä¢ [ ] Add retry logic for API calls with exponential backoff
‚Ä¢ [ ] Upsert chunks to Pinecone with metadata (chunk ID, text, section heading, page number, chunk position)
‚Ä¢ [ ] Implement progress bar for chunking process
‚Ä¢ [ ] Log progress and errors for verification
‚Ä¢ [ ] Create validation script to verify all chunks were properly embedded
‚Ä¢ [ ] Add dry-run mode for testing without API calls

üîç Semantic Search + RAG API
‚Ä¢ [ ] Implement /app/api/chat/route.ts as POST endpoint
‚Ä¢ [ ] Add input validation and sanitization
‚Ä¢ [ ] Implement rate limiting middleware (e.g., using upstash/ratelimit)
‚Ä¢ [ ] Accept user query as input
‚Ä¢ [ ] Add query length limits and validation
‚Ä¢ [ ] Embed user query with OpenAI
‚Ä¢ [ ] Implement caching for repeated queries
‚Ä¢ [ ] Perform similarity search in Pinecone
‚Ä¢ [ ] Add fallback to Supabase pgvector if Pinecone fails
‚Ä¢ [ ] Retrieve top 3‚Äì5 relevant chunks
‚Ä¢ [ ] Implement relevance threshold to filter low-quality matches
‚Ä¢ [ ] Construct a prompt with retrieved context and question
‚Ä¢ [ ] Add prompt engineering for better citation formatting
‚Ä¢ [ ] Call OpenAI GPT for streaming completion
‚Ä¢ [ ] Implement timeout handling for long-running requests
‚Ä¢ [ ] Return assistant response along with list of cited HR1 chunks
‚Ä¢ [ ] Add response time logging and monitoring

üí¨ Frontend Chat UI
‚Ä¢ [ ] Create ChatPage in /app/page.tsx
‚Ä¢ [ ] Implement responsive design for mobile/tablet/desktop
‚Ä¢ [ ] Display user/assistant messages in chat format
‚Ä¢ [ ] Add loading states with skeleton UI
‚Ä¢ [ ] Add error states with user-friendly messages
‚Ä¢ [ ] Input box for entering questions
‚Ä¢ [ ] Add character limit indicator
‚Ä¢ [ ] Implement "Enter to send, Shift+Enter for new line"
‚Ä¢ [ ] Use @vercel/ai useChat for request/response streaming
‚Ä¢ [ ] Add abort functionality for in-progress requests
‚Ä¢ [ ] Render assistant responses using react-markdown
‚Ä¢ [ ] Add syntax highlighting for any code blocks
‚Ä¢ [ ] Display citations below assistant messages with expandable section text
‚Ä¢ [ ] Add "Copy to clipboard" for responses
‚Ä¢ [ ] Implement chat history with localStorage
‚Ä¢ [ ] Add "Clear chat" functionality
‚Ä¢ [ ] Add accessibility features (ARIA labels, keyboard navigation)
‚Ä¢ [ ] Implement auto-scroll to latest message

üß™ Testing & Quality Assurance
‚Ä¢ [ ] Set up Jest and React Testing Library
‚Ä¢ [ ] Write unit tests for chunking algorithm
‚Ä¢ [ ] Write unit tests for API endpoint logic
‚Ä¢ [ ] Add integration tests for chat flow
‚Ä¢ [ ] Test error scenarios (API failures, rate limits)
‚Ä¢ [ ] Add performance benchmarks for search
‚Ä¢ [ ] Create test dataset of common HR1 questions

üîí Security & Performance
‚Ä¢ [ ] Implement CORS configuration
‚Ä¢ [ ] Add security headers (CSP, X-Frame-Options, etc.)
‚Ä¢ [ ] Set up API request logging (without sensitive data)
‚Ä¢ [ ] Implement cost tracking for OpenAI API usage
‚Ä¢ [ ] Add monitoring for API response times
‚Ä¢ [ ] Set up error tracking (e.g., Sentry)
‚Ä¢ [ ] Implement request throttling per user/IP

üöÄ Deployment
‚Ä¢ [ ] Create production environment variables
‚Ä¢ [ ] Deploy site to Vercel
‚Ä¢ [ ] Configure custom domain (if applicable)
‚Ä¢ [ ] Add required secrets (OpenAI, Pinecone keys) to Vercel environment
‚Ä¢ [ ] Set up preview deployments for PRs
‚Ä¢ [ ] Test chat interface end-to-end
‚Ä¢ [ ] Validate citation traceability
‚Ä¢ [ ] Create basic monitoring dashboard
‚Ä¢ [ ] Set up uptime monitoring

üìö Documentation
‚Ä¢ [ ] Create README.md with setup instructions
‚Ä¢ [ ] Document API endpoints and request/response formats
‚Ä¢ [ ] Add inline code comments for complex logic
‚Ä¢ [ ] Create user guide for the chat interface
‚Ä¢ [ ] Document chunking strategy and parameters

Stretch Goals (Optional)
‚Ä¢ [ ] Claude integration toggle
‚Ä¢ [ ] Sidebar or inline full HR1 viewer
‚Ä¢ [ ] Chat UI enhancements (e.g., citation highlighting)
‚Ä¢ [ ] Admin-only ingestion trigger (for future docs)
‚Ä¢ [ ] Export chat history as PDF/Markdown
‚Ä¢ [ ] Add voice input support
‚Ä¢ [ ] Implement semantic caching for similar queries
‚Ä¢ [ ] Add analytics for popular questions
‚Ä¢ [ ] Multi-language support
‚Ä¢ [ ] Dark mode toggle

Phase 2: Senatorial Value Alignment Layer (Optional)
Purpose
Compare the content of HR1 with campaign promises or stated values from each currently sitting Senate member, based on content scraped from their official or campaign websites.

Tasks
‚Ä¢ [ ] Scrape senate.gov member list and individual pages
‚Ä¢ [ ] Extract value/policy statements from .gov or campaign domains
‚Ä¢ [ ] Embed statements using OpenAI embeddings
‚Ä¢ [ ] Store with metadata: { name, party, district, text, url }
‚Ä¢ [ ] Enable semantic comparison between HR1 sections and member values
‚Ä¢ [ ] Add UI to explore alignment/conflict by rep, party, state, or issue

Future Enhancements
‚Ä¢ [ ] Score alignment/conflict as a semantic distance metric
‚Ä¢ [ ] Offer summary per representative's consistency with HR1
‚Ä¢ [ ] Track document freshness or cache updates

üèõÔ∏è State-Specific Impact Analysis (New Feature)
Purpose
Add state-specific metadata to HR1 chunks to enable users to filter content by how it impacts their state. This allows citizens to quickly find provisions that directly affect their state's economy, rights, infrastructure, or federal programs.

Phase 1: Analysis & Preparation
‚Ä¢ [ ] Export existing chunks from Pinecone to analyze patterns
‚Ä¢ [ ] Manually review 50-100 chunks to identify state impact patterns
‚Ä¢ [ ] Create classification criteria document defining "state impact"
‚Ä¢ [ ] Identify keyword patterns for pre-filtering (funding, grants, infrastructure, etc.)
‚Ä¢ [ ] Design prompt templates optimized for token efficiency
‚Ä¢ [ ] Set up cost tracking spreadsheet for API usage

Phase 2: Classification Pipeline Development
‚Ä¢ [ ] Create scripts/classify_state_impacts.ts
‚Ä¢ [ ] Implement chunk export from Pinecone with pagination
‚Ä¢ [ ] Build pre-filtering logic to exclude obvious non-impact chunks:
  - Procedural language (definitions, effective dates, etc.)
  - International relations without domestic impact
  - Generic federal regulations without state specificity
‚Ä¢ [ ] Implement two-stage classification:
  - Stage 1: Binary classification (has state impact: yes/no)
  - Stage 2: State identification for impacted chunks
‚Ä¢ [ ] Create batch processing with 5-10 chunks per API call
‚Ä¢ [ ] Add GPT-4o-mini integration for cost-effective classification
‚Ä¢ [ ] Implement regex/keyword matching for obvious cases:
  - Explicit state mentions
  - Federal funding formulas
  - Infrastructure projects by region
‚Ä¢ [ ] Add progress tracking and resumability
‚Ä¢ [ ] Create validation dataset for accuracy testing
‚Ä¢ [ ] Implement caching to avoid reprocessing
‚Ä¢ [ ] Add dry-run mode for testing

Phase 3: Pinecone Metadata Update
‚Ä¢ [ ] Design new metadata schema with "states" array field
‚Ä¢ [ ] Create backup of existing Pinecone data
‚Ä¢ [ ] Implement batch update logic for Pinecone vectors
‚Ä¢ [ ] Add verification step to ensure updates are successful
‚Ä¢ [ ] Create rollback capability in case of errors
‚Ä¢ [ ] Update data ingestion script for future chunks

Phase 4: Frontend State Filtering
‚Ä¢ [ ] Add state selector dropdown to Chat component
‚Ä¢ [ ] Implement all 50 states + DC + territories
‚Ä¢ [ ] Create "My State" preference with localStorage
‚Ä¢ [ ] Modify search query to include state metadata filter
‚Ä¢ [ ] Update prompt engineering to mention state context
‚Ä¢ [ ] Add visual indicator showing state filter is active
‚Ä¢ [ ] Create "nationwide impact" option for federal-level queries
‚Ä¢ [ ] Add state impact badges to citation display

Phase 5: Testing & Optimization
‚Ä¢ [ ] Test classification accuracy on validation set
‚Ä¢ [ ] Measure API costs and optimize prompts
‚Ä¢ [ ] Performance test filtered queries
‚Ä¢ [ ] User test state selection UX
‚Ä¢ [ ] Create documentation for state impact criteria

Cost Optimization Strategies
‚Ä¢ Use GPT-4o-mini ($0.15/1M input, $0.60/1M output tokens)
‚Ä¢ Aggressive pre-filtering to reduce chunks by 40-60%
‚Ä¢ Batch processing (5-10 chunks per API call)
‚Ä¢ Regex/keyword matching for obvious cases (save 20-30% API calls)
‚Ä¢ Cache all classifications to avoid reprocessing
‚Ä¢ Skip chunks under 100 tokens (usually boilerplate)

Estimated Costs
‚Ä¢ Assuming ~2000 chunks after filtering
‚Ä¢ ~200-400 API calls with batching
‚Ä¢ Estimated tokens: ~500K input, ~100K output
‚Ä¢ Total cost: ~$0.15 (very affordable!)

Implementation Order
1. Build and test classification pipeline locally
2. Process chunks in small batches to validate approach
3. Run full classification after validation
4. Update Pinecone metadata
5. Implement frontend changes
6. Deploy and monitor

Success Metrics
‚Ä¢ [ ] 90%+ classification accuracy on validation set
‚Ä¢ [ ] Total API costs under $1
‚Ä¢ [ ] State filtering reduces results by 60-80% on average
‚Ä¢ [ ] User feedback shows improved relevance

Notes
‚Ä¢ Use AWS credits only if Pinecone or embedding workloads exceed free tiers
‚Ä¢ Chunking and ingestion is one-time; no recurring job needed
‚Ä¢ Citations must be shown with each model response for transparency
‚Ä¢ Text chunks should be capped to ~1000 tokens with overlap to maintain coherence
‚Ä¢ Consider implementing a feedback mechanism for answer quality
‚Ä¢ Monitor costs closely, especially during development

Next Steps
‚Ä¢ [ ] Finalize clean HR1 plain text file
‚Ä¢ [ ] Run chunking + embedding script once to populate Pinecone index
‚Ä¢ [ ] Implement API route to power question answering
‚Ä¢ [ ] Build chat UI and test citation formatting
‚Ä¢ [ ] Conduct user testing with sample queries
